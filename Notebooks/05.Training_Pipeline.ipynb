{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573ab7ae",
   "metadata": {},
   "source": [
    "## Manual-Auto Process\n",
    "   - Prediction: manually\n",
    "   - Gradients Computations\n",
    "       - replace manually computed gradient with autograd\n",
    "   - Loss Computations: manually\n",
    "       - PyTorch Loss\n",
    "   - Parameter Updates: manually\n",
    "       - PyTorch Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c45c0",
   "metadata": {},
   "source": [
    "## General Train Pipeline\n",
    "\n",
    "01. Design model (input, output, forward pass with different layers)\n",
    "02. Construct loss and optimizer\n",
    "03. Training loop\n",
    "    - Forward = compute prediction and loss\n",
    "    - Backward = compute gradients\n",
    "    - Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dd10710",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import module\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn #neural network module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4192a348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction before training: f(5) = 0.000\n",
      "epoch  1 : w =  tensor(0.3000, requires_grad=True)  loss =  tensor(30., grad_fn=<MseLossBackward0>)\n",
      "epoch  11 : w =  tensor(1.6653, requires_grad=True)  loss =  tensor(1.1628, grad_fn=<MseLossBackward0>)\n",
      "epoch  21 : w =  tensor(1.9341, requires_grad=True)  loss =  tensor(0.0451, grad_fn=<MseLossBackward0>)\n",
      "epoch  31 : w =  tensor(1.9870, requires_grad=True)  loss =  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "epoch  41 : w =  tensor(1.9974, requires_grad=True)  loss =  tensor(6.7705e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch  51 : w =  tensor(1.9995, requires_grad=True)  loss =  tensor(2.6244e-06, grad_fn=<MseLossBackward0>)\n",
      "epoch  61 : w =  tensor(1.9999, requires_grad=True)  loss =  tensor(1.0176e-07, grad_fn=<MseLossBackward0>)\n",
      "epoch  71 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(3.9742e-09, grad_fn=<MseLossBackward0>)\n",
      "epoch  81 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(1.4670e-10, grad_fn=<MseLossBackward0>)\n",
      "epoch  91 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(5.0768e-12, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model: Weights to optimize and forward function\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "print(f'\\nPrediction before training: f(5) = {forward(5).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "# callable function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_predicted = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch ', epoch+1, ': w = ', w, ' loss = ', l)\n",
    "\n",
    "print(f'\\nPrediction after training: f(5) = {forward(5).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530cff5e",
   "metadata": {},
   "source": [
    "**Note:** accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f58ea4",
   "metadata": {},
   "source": [
    "## Automatic Process\n",
    "   - Prediction: PyTorch\n",
    "   - Gradients Computations\n",
    "       - replace manually computed gradient with autograd\n",
    "   - Loss Computations: manually\n",
    "       - PyTorch Loss\n",
    "   - Parameter Updates: manually\n",
    "       - PyTorch Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76202c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 4, #features: 1\n",
      "\n",
      "Prediction before training: f(5) = 0.640\n",
      "epoch  1 : w =  0.5179450511932373  loss =  tensor(28.2137, grad_fn=<MseLossBackward0>)\n",
      "epoch  11 : w =  1.7353107929229736  loss =  tensor(0.7314, grad_fn=<MseLossBackward0>)\n",
      "epoch  21 : w =  1.9318997859954834  loss =  tensor(0.0202, grad_fn=<MseLossBackward0>)\n",
      "epoch  31 : w =  1.9642748832702637  loss =  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "epoch  41 : w =  1.9702141284942627  loss =  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "epoch  51 : w =  1.9718796014785767  loss =  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "epoch  61 : w =  1.9728366136550903  loss =  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "epoch  71 : w =  1.9736592769622803  loss =  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "epoch  81 : w =  1.974440574645996  loss =  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "epoch  91 : w =  1.975196361541748  loss =  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Prediction after training: f(5) = 9.950\n"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "# f = w * x \n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples, watch the shape!\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(f'#samples: {n_samples}, #features: {n_features}')\n",
    "# 0) create a test sample\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# we can call this model with samples X\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "\n",
    "print(f'\\nPrediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
    "\n",
    "print(f'\\nPrediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a3535",
   "metadata": {},
   "source": [
    "**Note:** don't have to worry about to construct the learning model anymore, but need to know what optimizer and loss function should use. The prediction outcome gets better with proper learning rate and iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97beae99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
